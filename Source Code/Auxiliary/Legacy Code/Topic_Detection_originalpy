import lda
import sys
import pymongo
import numpy as np
from datetime import datetime
from nltk.tokenize import RegexpTokenizer
from getch import getch, pause, pause_exit
import csv


LogDir = "./Source Code/Logs/"
LDADir = "./Source Code/LDA/"

def main():
    #createOrderedIDsList()
    readTweets()

    WriteLog('.-= New LDA Iteration =-.\n')
    WriteLog('Creating the Client\n')
    client = pymongo.MongoClient() #Lack of arguments defaults to localhost:27017
    WriteLog('Connecting to mongorefcon\n')
    db = client['mongorefcon']
    WriteLog('Using StemmedTweets\n')
    StemmedTweets = db['StemmedTweets']

    print("Getting the Titles")
    WriteLog('Getting the Titles\n')
    titles = get_titles(StemmedTweets, "stemmed")

    ids = get_titles(StemmedTweets, "id")  # ids of the tweets

    print("Creating the Vocabulary")
    WriteLog('Creating the Vocabulary\n')
    vocab = get_vocabulary(titles)

    #print(vocab)

    print("Creating the Frequency Table")
    WriteLog('Creating the Frequency Table\n')
    freqtable = get_frequency_table(titles, vocab)

    print("Running the LDA")
    WriteLog('Running the LDA\n')
    runLDA(titles, vocab, freqtable, ids)
    WriteLog('Finished successfully\n\n\n')
    print("Press any key to terminate.")
    getch()

def readTweets():
    client = pymongo.MongoClient() #Lack of arguments defaults to localhost:27017
    db = client['mongorefcon']
    StemmedTweets = db['StemmedTweets']

    LDA_csv_reader = open(LDADir + 'FilteredLDA_top10.csv')

    for row in LDA_csv_reader:
        line_items = row.replace('\n','').split('\t')
        tweet_id = line_items[0]

        for RawTweet in StemmedTweets.find({ "id": tweet_id }):
            tweet_text = str(RawTweet['orig_tweet'].encode('utf8'))
            print("Topic id = " + line_items[1] + '\t' + tweet_text)

    y

def createOrderedIDsList():
    LDA_csv_writer = open(LDADir + 'FilteredLDA.csv', 'a')
    LDA_csv_reader = open(LDADir + 'OrderedLDA.csv')

    read_count = 0
    topic_id = 0

    for row in LDA_csv_reader:
        line_items = row.replace('\n','').split(',')
        line_items[1] = int(line_items[1])
        read_count += 1

        if read_count <= 100:
            LDA_csv_writer.write(str(line_items[0]) + '\n')
            #LDA_csv_writer.write(str(line_items[0]) + '\t' + str(line_items[1]) + '\t' + str(line_items[2]) + '\n')

        if topic_id != line_items[1]:
            topic_id +=1
            read_count = 0

    LDA_csv_writer.close()


def runLDA(titles, words, freqtable, ids):
    nof_topics = 25
    n_iter=1000
    random_state=1
    WriteLog("nof_topics = " + str(nof_topics) + "\n" + "n_topics = " + str(nof_topics) +  "\n" + "random_state = " + str(random_state) + "\n")

    WriteLog("Creating the Model\n")
    model = lda.LDA(n_topics=nof_topics, n_iter=1000, random_state=1)
    WriteLog("Fitting the Frequency Table\n")

    doc_topic = model.fit_transform(freqtable)

    WriteLog("Getting topic_word\n")
    topic_word = model.topic_word_  # model.components_ also works
    n_top_words = 10
    WriteLog("n_top_words = " + str(n_top_words) + "\n")

    WriteLog("Creating the Vocabulary\n")
    vocab = list(words.keys())


    #for i, topic_dist in enumerate(topic_word):
    #    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    #    print(' \n Topic {}: {}'.format(i, ' '.join(topic_words)))
    #    WriteLog('\n\n Topic {}: {}'.format(i, ' '.join(topic_words)))

    LDA_csv = open(LDADir + 'LDA.csv', 'a')

    doc_topic = model.doc_topic_
    for i in range(len(ids)):
        Content_part1 = ids[i]
        Content_part2 = doc_topic[i].argmax()
        Content_part3 = doc_topic[i][Content_part2]

        LDA_csv.write(str(Content_part1) + ' ' + str(Content_part2) + ' ' + str(Content_part3) + '\n')

    LDA_csv.close()


def get_frequency_table(titles, vocab):
    tokenizer = RegexpTokenizer(r'\w+')

    freqtable = np.ndarray(shape=(len(titles),len(vocab)), dtype=int, order='C')
    freqtable.fill(0)

    for i in range(0,len(titles)):
        raw = titles[i].lower()
        tokens = tokenizer.tokenize(raw)
        for token in tokens:
            index = vocab[token]
            freqtable[i][index] +=1

    return freqtable

def WriteLog(text):
    LDA_Log_Write = open(LogDir + 'LDA.log', 'a')
    LDA_Log_Write.write(text)
    LDA_Log_Write.close()


def get_vocabulary(doc_set):
    tokenizer = RegexpTokenizer(r'\w+')

    distinctwords = {}
    i = 0

    # loop through document list
    for text in doc_set:
        raw = text.lower()
        tokens = tokenizer.tokenize(raw)

        for word in tokens:
            if word not in distinctwords:
                #distinctwords.append(word)
                distinctwords[word] = i
                i = i + 1

    return distinctwords

def get_titles(collection, TweetTextFieldName):
    doc_set = []
    try:
        for RawTweet in collection.find(): #.limit(1000): #in case we need to continue from a particular place in the collection, add the appropriate skip argument on find()
            try:
                tmpTweet = RawTweet[TweetTextFieldName]
                doc_set.append(tmpTweet)
            except Exception as ex:
                print('Print error\n' + 'Time of Error: ' + str(datetime.now()) + '\n' + str(ex) + '\n')
                continue

    except Exception as ex:
        print('Print error\n' + 'Time of Error: ' + str(datetime.now()) + '\n' + str(ex) + '\n')

    return doc_set

main()
